{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import boto3\n",
    "import io\n",
    "import pandas as pd\n",
    "import sqlalchemy as sa \n",
    "from sqlalchemy import  MetaData\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.engine.url import URL\n",
    "from sqlalchemy.schema import MetaData\n",
    "from sqlalchemy.engine.url import URL\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from dotenv import load_dotenv\n",
    "from base import OtuCount, FeatureCountExtendedView\n",
    "import requests\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import utils\n",
    "import importlib\n",
    "importlib.reload(utils)\n",
    "from io import StringIO\n",
    "# from sklearn.preprocessing import normalize\n",
    "# import skbio.diversity\n",
    "# from utils import df_to_parquet, resp_to_df\n",
    "# from skbio import DistanceMatrix\n",
    "# from skbio.stats.ordination import pcoa, pcoa_biplot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar variables de entorno\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def resp_to_df(resp):\n",
    " \n",
    "    df = pd.read_json(resp)\n",
    "    return df\n",
    "\n",
    "def df_to_parquet(df):\n",
    " # df_otu_con_project es tu DataFrame\n",
    "    parquet_buffer = df.to_parquet( engine='pyarrow', compression='snappy')\n",
    "    return parquet_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de la conexión a la base de datos\n",
    "def get_engine():\n",
    "    url = URL.create(\n",
    "        drivername=os.getenv('DB_DRIVER'),\n",
    "        host=os.getenv('DB_HOST'),\n",
    "        port=os.getenv('DB_PORT'),\n",
    "        database=os.getenv('DB_NAME'),\n",
    "        username=os.getenv('DB_USER'),\n",
    "        password=os.getenv('DB_PASSWORD')\n",
    "    )\n",
    "    engine =sa.create_engine(url)\n",
    "    return engine\n",
    "\n",
    "def client(client):\n",
    "    return boto3.client(\n",
    "    client,\n",
    "    aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),\n",
    "    aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY'),\n",
    "    region_name=os.getenv('S3_REGION')\n",
    "    )\n",
    "    \n",
    "    engine = get_engine()\n",
    "# Creación de MetaData y Declarative Base\n",
    "metadata = MetaData()\n",
    "Base = declarative_base(metadata=metadata)\n",
    "# Creación de la sesión\n",
    "Session = sessionmaker(bind=get_engine())\n",
    "session = Session()\n",
    "s3 = client('s3')\n",
    "# Configurar el cliente de Athena\n",
    "athena_client = client('athena')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projectid = 'E294'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_key = \"_\".join([\"metadata\", projectid])\n",
    "cache_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = f'http://localhost:3000/api/metadata/{projectid}'\n",
    "\n",
    "\n",
    "headers = {\n",
    "    'Authorization': 'Bearer eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCIsImtpZCI6InlELVZ2aGFXVjRCVWhWX2VYSnpmdiJ9.eyJpc3MiOiJodHRwczovL2Rldi13a20wdGtkdC51cy5hdXRoMC5jb20vIiwic3ViIjoiUW9rOWxzZ2duaVFWRDFTV3FUdHZiRmpFMmlRMGxFN09AY2xpZW50cyIsImF1ZCI6Imh0dHBzOi8vc2l3YS5iaW8iLCJpYXQiOjE3MDY0OTMzOTAsImV4cCI6MTcwNjU3OTc5MCwiYXpwIjoiUW9rOWxzZ2duaVFWRDFTV3FUdHZiRmpFMmlRMGxFN08iLCJndHkiOiJjbGllbnQtY3JlZGVudGlhbHMifQ.DzKBMSOjij2TQsAdvdNygMSsG5uObx4p0Bnii-N8RQ0tJcRqjMsdHhkHLXW4d8V-rua2B8-lJ1DiqlfP0n608RAfisNSMg3Jjhp-qSu4KE6fC9OmHHvhbLpt9RrYkgb2KELEtRbE6OQZt8zoebZWxuWDOeBJ6ff-X_I0KQx4629i-89Lj4AUKDkDOoOudyQDu37p-10KxicNTDxYo8g5clSHrqRQqcOLhGwNVBfP6OH8U6qPOZnuW5NBFxVIZ4jHtZXu_m_6i9eCZvA9IivoHp_Y7yx0nZUR1Rin8lU0DBJn1UcyzX750TLWTfCIQmpBLVsjj27_mW3KD3wof4vMlA',\n",
    "}\n",
    "response = requests.get(\n",
    "    url,\n",
    "    headers=headers,\n",
    ")\n",
    "\n",
    "\n",
    "result = response.json()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta = pd.DataFrame(result[\"data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp_parquet = df_to_parquet(df_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(resp_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_s3(bucket_name, object_name, data):\n",
    "    try:\n",
    "        s3.put_object(Bucket=bucket_name, Key=object_name, Body=data)\n",
    "        return {\"message\": \"Archivo subido exitosamente\"}\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_s3(bucket_name, object_name):\n",
    "    try:\n",
    "        \n",
    "        # Lee el archivo desde S3\n",
    "        response = s3.get_object(Bucket=bucket_name, Key=object_name)\n",
    "\n",
    "        # Lee los datos del contenido del archivo\n",
    "        content = response['Body'].read()\n",
    "\n",
    "        # Puedes convertir los datos en un DataFrame si es un archivo Parquet, por ejemplo\n",
    "        df = pd.read_csv(io.BytesIO(content))\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = pd.read_csv(\"E335_extra.csv\")\n",
    "csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = csv.to_csv(index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = \"siwaexperiments\"\n",
    "file_name = f\"{projectid}/{projectid}_extra.csv\"\n",
    "upload_response = upload_to_s3(bucket_name, file_name, data)\n",
    "\n",
    "print(upload_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = \"siwaexperiments\"\n",
    "file_name = f\"{projectid}/{projectid}_extra.csv\"\n",
    "upload_response = read_from_s3(bucket_name, file_name)\n",
    "upload_response = upload_response.rename(columns={'SampleID':'fullSampleId',\n",
    "                                   })\n",
    "\n",
    "print(upload_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(upload_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = df_meta.merge(upload_response, on=\"fullSampleId\", how=\"left\")\n",
    "full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp_parquet = df_to_parquet(full_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(resp_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = \"siwaparquets\"\n",
    "file_name = f\"meta/{projectid}.parquet\"\n",
    "upload_response = upload_to_s3(bucket_name, file_name, resp_parquet)\n",
    "\n",
    "print(upload_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bearer = 'eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCIsImtpZCI6InlELVZ2aGFXVjRCVWhWX2VYSnpmdiJ9.eyJpc3MiOiJodHRwczovL2Rldi13a20wdGtkdC51cy5hdXRoMC5jb20vIiwic3ViIjoiUW9rOWxzZ2duaVFWRDFTV3FUdHZiRmpFMmlRMGxFN09AY2xpZW50cyIsImF1ZCI6Imh0dHBzOi8vc2l3YS5iaW8iLCJpYXQiOjE3MDYyODQ2MzMsImV4cCI6MTcwNjM3MTAzMywiYXpwIjoiUW9rOWxzZ2duaVFWRDFTV3FUdHZiRmpFMmlRMGxFN08iLCJndHkiOiJjbGllbnQtY3JlZGVudGlhbHMifQ.QMyRp9VDIlY5pyS8BlrsEwsvm4kiwSeaXDc_Rrm4RwXDNxUl_FoMVZNHrM-LavP6W_V28hHNUymTudOl7qlABteG8ydv3v4Z4lTrPzdP3xvF75b9jpHxf6SNCRfaFVr8MUSl41H4qCxHErtaXBDix_HzjtMMJhzLfnkKRodmQKH2F-HZlsl2bfc1S7gignANnUglJ6r4qMbyQqpVczNhCX1ONTbKBC05Bnb1BuSfzPoRp-jqxZt6oSXTqtEh9n4UrPdua-SCIcGZGRrEZxLHqHT479lN5ataG0FP7aFURKvQwzTRhlABcP-n5f0WuiWvy6wz-PWk4KFc78pZpDlr8Q'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"'Bearer {Bearer}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = f'http://localhost:3000/api/otucount/{projectid}'\n",
    "\n",
    "\n",
    "headers = {\n",
    "    'Authorization': 'Bearer eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCIsImtpZCI6InlELVZ2aGFXVjRCVWhWX2VYSnpmdiJ9.eyJpc3MiOiJodHRwczovL2Rldi13a20wdGtkdC51cy5hdXRoMC5jb20vIiwic3ViIjoiUW9rOWxzZ2duaVFWRDFTV3FUdHZiRmpFMmlRMGxFN09AY2xpZW50cyIsImF1ZCI6Imh0dHBzOi8vc2l3YS5iaW8iLCJpYXQiOjE3MDY0OTMzOTAsImV4cCI6MTcwNjU3OTc5MCwiYXpwIjoiUW9rOWxzZ2duaVFWRDFTV3FUdHZiRmpFMmlRMGxFN08iLCJndHkiOiJjbGllbnQtY3JlZGVudGlhbHMifQ.DzKBMSOjij2TQsAdvdNygMSsG5uObx4p0Bnii-N8RQ0tJcRqjMsdHhkHLXW4d8V-rua2B8-lJ1DiqlfP0n608RAfisNSMg3Jjhp-qSu4KE6fC9OmHHvhbLpt9RrYkgb2KELEtRbE6OQZt8zoebZWxuWDOeBJ6ff-X_I0KQx4629i-89Lj4AUKDkDOoOudyQDu37p-10KxicNTDxYo8g5clSHrqRQqcOLhGwNVBfP6OH8U6qPOZnuW5NBFxVIZ4jHtZXu_m_6i9eCZvA9IivoHp_Y7yx0nZUR1Rin8lU0DBJn1UcyzX750TLWTfCIQmpBLVsjj27_mW3KD3wof4vMlA',\n",
    "}\n",
    "response = requests.get(\n",
    "    url,\n",
    "    headers=headers,\n",
    ")\n",
    "\n",
    "\n",
    "result_otu = response.json()\n",
    "result_otu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "otutable = pd.DataFrame(result_otu[\"data\"])\n",
    "otutable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pivotear_datos(df):\n",
    "    \"\"\"\n",
    "    Pivotea los datos de un DataFrame.\n",
    "\n",
    "    Parámetros:\n",
    "    df (DataFrame): DataFrame que contiene las columnas 'value', 'otu' y 'sampleId'.\n",
    "\n",
    "    Devuelve:\n",
    "    DataFrame: DataFrame pivoteado con 'otu' como índice y 'sampleId' como columnas.\n",
    "    \"\"\"\n",
    "\n",
    "    # Crear una tabla pivote\n",
    "    datos_pivoteados = (\n",
    "        df.pivot_table(values='value', index='otu', columns='sampleId')\n",
    "          .fillna(0)\n",
    "          .rename_axis(index=None, columns=None)\n",
    "    )\n",
    "\n",
    "    # Restablecer el índice\n",
    "    datos_pivoteados = datos_pivoteados.reset_index()\n",
    "\n",
    "    return datos_pivoteados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "otu_pivot = pivotear_datos(otutable)\n",
    "otu_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "otu_parquet = df_to_parquet(otu_pivot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = \"siwaparquets\"\n",
    "file_name = f\"otu/{projectid}/{projectid}.parquet\"\n",
    "upload_response = upload_to_s3(bucket_name, file_name, otu_parquet)\n",
    "\n",
    "print(upload_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hasta este punto se crearon los archivos en parquet a partir de consultas al endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ahora se consultan archivos parquet en s3 con athena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_athena_table(athena_client, formatted_columns, location,projectid, data):\n",
    "    # Formatear los nombres de las columnas\n",
    "\n",
    "    # Definir la consulta DDL para crear la tabla\n",
    "    ddl_query = f\"\"\"\n",
    "        CREATE EXTERNAL TABLE IF NOT EXISTS siwa_adb.tba_{data}_{projectid} (\n",
    "\n",
    "            {formatted_columns}\n",
    "        )\n",
    "        STORED AS PARQUET\n",
    "        LOCATION {location}\n",
    "        TBLPROPERTIES ('classification'='parquet');\n",
    "    \"\"\"\n",
    "\n",
    "    # Configurar el bucket de S3 para los resultados de la consulta\n",
    "    output_location = f's3://siwaparquets/outputAthenas/{projectid}/{data}/'\n",
    "\n",
    "    # Ejecutar la consulta DDL\n",
    "    response = athena_client.start_query_execution(\n",
    "        QueryString=ddl_query,\n",
    "        ResultConfiguration={\n",
    "            'OutputLocation': output_location,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Obtener el ID de ejecución de la consulta\n",
    "    query_execution_id = response['QueryExecutionId']\n",
    "\n",
    "    print(f\"ID de ejecución de la consulta: {query_execution_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_columns = \", \".join([f\"{col} double\" for col in otu_pivot.columns[1:]])\n",
    "formatted_columns = \"index string, \" + formatted_columns\n",
    "formatted_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "otu_pivot.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"otu\"\n",
    "location = f\"'s3://siwaparquets/otu/{projectid}/'\"\n",
    "create_athena_table(athena_client, formatted_columns, location, projectid, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def format_columns(dataframe):\n",
    "    formatted_columns = \", \".join([\n",
    "        f\"{col} DOUBLE\" if dtype == 'float64' else f\"{col} STRING\"\n",
    "        for col, dtype in zip(dataframe.columns, dataframe.dtypes)\n",
    "    ])\n",
    "    return formatted_columns\n",
    "\n",
    "\n",
    "formatted_columns = format_columns(full_data)\n",
    "formatted_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"meta\"\n",
    "location = f\"'s3://siwaparquets/meta/'\"\n",
    "create_athena_table(athena_client, formatted_columns, location, projectid, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ya se crearon las tablas en la base de datos de athena, lo que sigue ahora es consumir de esa base de datos para calcular alpha y beta diversidad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_s3(bucket_name, object_name):\n",
    "    try:\n",
    "        s3 = boto3.client('s3')\n",
    "        \n",
    "        # Lee el archivo desde S3\n",
    "        response = s3.get_object(Bucket=bucket_name, Key=object_name)\n",
    "\n",
    "        # Lee los datos del contenido del archivo\n",
    "        content = response['Body'].read()\n",
    "\n",
    "        # Puedes convertir los datos en un DataFrame si es un archivo Parquet, por ejemplo\n",
    "        df = pd.read_parquet(io.BytesIO(content))\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_athena_query(athena_client, query, database, s3_output):\n",
    "    \"\"\"\n",
    "    Ejecuta una consulta SQL en Athena y devuelve el resultado.\n",
    "\n",
    "    :param athena_client: Cliente de Athena.\n",
    "    :param query: Consulta SQL a ejecutar.\n",
    "    :param database: Base de datos de Athena.\n",
    "    :param s3_output: Ubicación de S3 para los resultados de la consulta.\n",
    "    :return: Resultado de la consulta o None si la consulta falla.\n",
    "    \"\"\"\n",
    "\n",
    "    # Configuración de la ejecución de la consulta\n",
    "    query_execution = athena_client.start_query_execution(\n",
    "        QueryString=query,\n",
    "        QueryExecutionContext={'Database': database},\n",
    "        ResultConfiguration={'OutputLocation': s3_output}\n",
    "    )\n",
    "\n",
    "    # Esperar a que la consulta se complete\n",
    "    query_execution_id = query_execution['QueryExecutionId']\n",
    "\n",
    "    while True:\n",
    "        # Obtener el estado actual de la ejecución de la consulta\n",
    "        query_status = athena_client.get_query_execution(\n",
    "            QueryExecutionId=query_execution_id\n",
    "        )['QueryExecution']['Status']['State']\n",
    "\n",
    "        if query_status in ['SUCCEEDED', 'FAILED', 'CANCELLED']:\n",
    "            break\n",
    "\n",
    "        print(f\"Estado de la consulta: {query_status}. Esperando...\")\n",
    "        time.sleep(5)  # Espera 5 segundos antes de volver a verificar el estado\n",
    "\n",
    "    if query_status == 'FAILED' or query_status == 'CANCELLED':\n",
    "        raise Exception(f'La consulta de Athena no se completó. Estado: {query_status}')\n",
    "\n",
    "    # Utiliza un paginador para manejar los resultados de la consulta\n",
    "    paginator = athena_client.get_paginator('get_query_results')\n",
    "    page_iterator = paginator.paginate(QueryExecutionId=query_execution_id)\n",
    "\n",
    "    # Recorre cada página (cada una con hasta 1000 registros) y procesa los resultados\n",
    "    all_results = []\n",
    "    for page in page_iterator:\n",
    "        for row in page['ResultSet']['Rows']:\n",
    "            # Procesa cada fila\n",
    "            all_results.append(row)\n",
    "\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'otu'\n",
    "projectid = 'E294'\n",
    "query = f\"\"\"SELECT * FROM tba_{data}_{projectid}\"\"\"\n",
    "database = 'siwa_adb'\n",
    "s3_output = f's3://siwaparquets/outputAthenas/{projectid}/{data}/'\n",
    "resultados_otu = run_athena_query(athena_client,query, database, s3_output)\n",
    "resultados_otu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'meta'\n",
    "projectid = 'E335'\n",
    "query = f\"\"\"SELECT alphaObserved, alphaShannon, fullSampleId, sampleLocation FROM tba_{data}_{projectid}\"\"\"\n",
    "database = 'siwa_adb'\n",
    "s3_output = f's3://siwaparquets/outputAthenas/{projectid}/{data}/'\n",
    "resultados_meta = run_athena_query(athena_client,query, database, s3_output)\n",
    "resultados_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertir_a_dataframe(resultados):\n",
    "    \"\"\"\n",
    "    Convierte una lista de resultados en un DataFrame de Pandas.\n",
    "\n",
    "    Parámetros:\n",
    "    resultados (list): Lista de resultados, donde cada resultado es un diccionario con claves 'Data'.\n",
    "\n",
    "    Devuelve:\n",
    "    DataFrame: DataFrame de Pandas construido a partir de los resultados.\n",
    "    \"\"\"\n",
    "\n",
    "    # Verificar si la primera fila contiene los nombres de las columnas\n",
    "    if all('VarCharValue' in d for d in resultados[0]['Data']):\n",
    "        column_names = [d['VarCharValue'] for d in resultados[0]['Data']]\n",
    "    else:\n",
    "        raise KeyError(\"La primera fila no contiene los nombres de las columnas\")\n",
    "\n",
    "    # Crear una lista de diccionarios para las filas de datos\n",
    "    rows = []\n",
    "    for row in resultados[1:]:\n",
    "        # Verificar si cada fila tiene la clave 'VarCharValue'\n",
    "        if all('VarCharValue' in d for d in row['Data']):\n",
    "            row_data = [d['VarCharValue'] for d in row['Data']]\n",
    "            row_dict = dict(zip(column_names, row_data))\n",
    "            rows.append(row_dict)\n",
    "        else:\n",
    "            # Aquí puedes decidir cómo manejar filas que no tienen 'VarCharValue'\n",
    "            print(\"Fila ignorada o incompleta detectada\")\n",
    "\n",
    "    # Crear el DataFrame\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "otuFinal = convertir_a_dataframe(resultados_otu)\n",
    "otuFinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metaFinal = convertir_a_dataframe(resultados_meta)\n",
    "metaFinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un objeto DistanceMatrix\n",
    "dm = DistanceMatrix(otuFinal, ids=etiquetas_muestras)\n",
    "\n",
    "# Realizar PCoA\n",
    "pcoa_results = pcoa(dm)\n",
    "\n",
    "# Obtener las coordenadas PCoA\n",
    "pcoa_coords = pcoa_results.samples[['PC1', 'PC2']]  # Ajustar según las PCs que quieras visualizar\n",
    "\n",
    "\n",
    "metaFinal.set_index('SampleID', inplace=True)\n",
    "\n",
    "# Unir los DataFrames\n",
    "df_unido = pcoa_coords.merge(metaFinal, left_index=True, right_index=True)\n",
    "\n",
    "# Mostrar el DataFrame resultante\n",
    "print(df_unido)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "athena-aws",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
