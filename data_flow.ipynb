{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import sqlalchemy as sa \n",
    "from sqlalchemy import  MetaData\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.engine.url import URL\n",
    "from sqlalchemy.schema import MetaData\n",
    "from sqlalchemy.engine.url import URL\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from dotenv import load_dotenv\n",
    "from base import FeatureCountExtendedView, SampleMetadataExtendedView\n",
    "import logging\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar el logging\n",
    "logging.basicConfig(filename='otus_projects_upload2.log', level=logging.INFO, \n",
    "                    format='%(asctime)s:%(levelname)s:%(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargar variables de entorno\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de la conexión a la base de datos\n",
    "def get_engine():\n",
    "    url = URL.create(\n",
    "        drivername=os.getenv('DB_DRIVER'),\n",
    "        host=os.getenv('DB_HOST'),\n",
    "        port=os.getenv('DB_PORT'),\n",
    "        database=os.getenv('DB_NAME'),\n",
    "        username=os.getenv('DB_USER'),\n",
    "        password=os.getenv('DB_PASSWORD')\n",
    "    )\n",
    "    engine =sa.create_engine(url)\n",
    "    return engine\n",
    "\n",
    "engine = get_engine()\n",
    "# Creación de MetaData y Declarative Base\n",
    "metadata = MetaData()\n",
    "Base = declarative_base(metadata=metadata)\n",
    "# Creación de la sesión\n",
    "Session = sessionmaker(bind=get_engine())\n",
    "session = Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def client(client):\n",
    "    return boto3.client(\n",
    "    client,\n",
    "    aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),\n",
    "    aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY'),\n",
    "    region_name=os.getenv('S3_REGION')\n",
    "    )\n",
    "# Configurar el cliente de s3\n",
    "s3_client = client('s3')\n",
    "# Configurar el cliente de Athena\n",
    "athena_client = client('athena')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_s3(bucket_name, object_name, s3_client):\n",
    "    try:\n",
    "        \n",
    "        # Lee el archivo desde S3\n",
    "        response = s3_client.get_object(Bucket=bucket_name, Key=object_name)\n",
    "\n",
    "        # Lee los datos del contenido del archivo\n",
    "\n",
    "        df = pd.read_csv(io.BytesIO(response['Body'].read()))\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pivotear_datos(df):\n",
    "    \"\"\"\n",
    "    Pivotea los datos de un DataFrame.\n",
    "\n",
    "    Parámetros:\n",
    "    df (DataFrame): DataFrame que contiene las columnas 'value', 'otu' y 'sampleId'.\n",
    "\n",
    "    Devuelve:\n",
    "    DataFrame: DataFrame pivoteado con 'otu' como índice y 'sampleId' como columnas.\n",
    "    \"\"\"\n",
    "\n",
    "    # Crear una tabla pivote\n",
    "    datos_pivoteados = (\n",
    "        df.pivot_table(values='value', index='otu', columns='sampleId')\n",
    "          .fillna(0)\n",
    "          .rename_axis(index=None, columns=None)\n",
    "    )\n",
    "\n",
    "    # Restablecer el índice\n",
    "    datos_pivoteados = datos_pivoteados.reset_index()\n",
    "\n",
    "    return datos_pivoteados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_project_parquet_to_s3(projectid, parquet, s3_client, bucket_name, dataType):\n",
    "    \"\"\"\n",
    "    Sube un archivo Parquet a un bucket de S3 específico basado en el ID del proyecto.\n",
    "\n",
    "    :param projectid: El ID del proyecto para el cual se subirá el archivo.\n",
    "    :param otu_parquet: El contenido del archivo Parquet o la ruta al archivo a subir.\n",
    "    :param s3_client: La instancia del cliente S3 para realizar la subida.\n",
    "    \"\"\"\n",
    "    file_name = f\"{dataType}/{projectid}/{projectid}.parquet\"\n",
    "    try:\n",
    "        s3_client.put_object(Bucket=bucket_name, Key=file_name, Body=parquet)\n",
    "        return {\"message\": \"Archivo subido exitosamente\"}\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_parquet(df):\n",
    " # df_otu_con_project es tu DataFrame\n",
    "    parquet_buffer = df.to_parquet( engine='pyarrow', compression='snappy')\n",
    "    return parquet_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_projectid_all():\n",
    "    \"\"\"Consulta a la tabla FeatureCountExtendedView para obtener una lista de IDs de proyectos existentes.\"\"\"\n",
    "    # Ejecutar la consulta directamente y obtener los projectId como un dataframe\n",
    "    df = pd.read_sql(session.query(FeatureCountExtendedView.projectId).statement, session.bind)\n",
    "    \n",
    "    # Filtrar los projectId nulos y obtener una lista única de projectId\n",
    "    project_ids = df['projectId'].dropna().unique().tolist()\n",
    "    \n",
    "    return project_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def get_feature_otu(projectId):\n",
    "    \"\"\"Consulta a la tabla otu para obtener los datos de un project especifico\"\"\"\n",
    "    query = session.query(\n",
    "        FeatureCountExtendedView.value, \n",
    "        FeatureCountExtendedView.otu, \n",
    "        FeatureCountExtendedView.sampleId\n",
    "    ).filter(FeatureCountExtendedView.projectId == projectId)\n",
    "    df = pd.read_sql(query.statement, session.bind)\n",
    "    \n",
    "    return df.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata_by_project_id(project_id=None):\n",
    "    \"\"\"Consulta a la tabla Microbiome con un límite especificado y opcionalmente filtra por projectId.\"\"\"\n",
    "        # Iniciar la consulta\n",
    "    query = session.query(SampleMetadataExtendedView)\n",
    "\n",
    "    # Filtrar por runId si se proporciona\n",
    "    if project_id:\n",
    "        query = query.filter(SampleMetadataExtendedView.projectId == project_id)\n",
    "\n",
    "    # Aplicar el límite\n",
    "\n",
    "\n",
    "    # Ejecutar la consulta y convertir el resultado en un diccionario\n",
    "    df = pd.read_sql(query.statement, session.bind)\n",
    "    return df.to_dict(orient='records')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_updated_projects(s3_client, s3_bucket, projects_parquet_key):\n",
    "    try:\n",
    "        response = s3_client.get_object(Bucket=s3_bucket, Key=projects_parquet_key)\n",
    "        buffer = BytesIO(response['Body'].read())\n",
    "        # Use the buffer to read the Parquet file\n",
    "        updated_projects_df = pd.read_parquet(buffer)\n",
    "        updated_projects_set = set(updated_projects_df['project_id'])\n",
    "        return updated_projects_set\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"No se pudo leer el archivo de proyectos actualizados desde S3: {e}\")\n",
    "        return set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_otus_for_project(project_id, s3_client, s3_bucket):\n",
    "    try:\n",
    "        project_data = pd.DataFrame(get_feature_otu(project_id))\n",
    "        if project_data.empty:\n",
    "            logging.warning(f'Proyecto otu {project_id}: No se encontraron datos.')\n",
    "            return\n",
    "\n",
    "        otu_pivot = pivotear_datos(project_data)\n",
    "        otu_parquet = df_to_parquet(otu_pivot)\n",
    "        upload_project_parquet_to_s3(project_id, otu_parquet, s3_client, s3_bucket, 'otu')\n",
    "        logging.info(f'Proyecto otu {project_id}: Datos subidos correctamente a S3.')\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logging.error(f'Proyecto otu {project_id}: Error al procesar - {e}')\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def update_projects_record(s3_client, s3_bucket, projects_parquet_key, updated_projects_set):\n",
    "    updated_projects_df = pd.DataFrame(list(updated_projects_set), columns=['project_id'])\n",
    "    updated_projects_parquet = df_to_parquet(updated_projects_df)\n",
    "    s3_client.put_object(Bucket=s3_bucket, Key=projects_parquet_key, Body=updated_projects_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def upload_parquets_otus_for_projects(project_list, s3_client):\n",
    "    s3_bucket = 'siwaparquets'\n",
    "    projects_parquet_key = 'projects/projects_record/otu.parquet'\n",
    "    \n",
    "    updated_projects_set = read_updated_projects(s3_client, s3_bucket, projects_parquet_key)\n",
    "\n",
    "    for project_id in project_list:\n",
    "        if project_id in updated_projects_set:\n",
    "            logging.info(f'Proyecto otus {project_id}: El archivo ya está en S3 y no se subirá nuevamente.')\n",
    "            continue\n",
    "\n",
    "        if process_otus_for_project(project_id, s3_client, s3_bucket):\n",
    "            updated_projects_set.add(project_id)\n",
    "\n",
    "    update_projects_record(s3_client, s3_bucket, projects_parquet_key, updated_projects_set)\n",
    "\n",
    "    return \"Proceso completado con éxito.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_data_if_needed(project_data, extra_data):\n",
    "    # Verificar si extra_data está vacío o es nulo antes de proceder\n",
    "    if extra_data.empty:\n",
    "        return project_data\n",
    "    else:\n",
    "        # Estandarizar los nombres de las columnas para ignorar mayúsculas/minúsculas\n",
    "        project_data.columns = project_data.columns.str.lower()\n",
    "        extra_data.columns = extra_data.columns.str.lower()\n",
    "\n",
    "        # Realizar el merge\n",
    "        return project_data.merge(extra_data, on=\"sampleid\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_project(project_id, s3_client, s3_bucket_extra_data, bucket_name):\n",
    "    project_data = pd.DataFrame(get_metadata_by_project_id(project_id))\n",
    "    if not project_data.empty:\n",
    "        file_name = f\"{project_id}/{project_id}_extra.csv\"\n",
    "        extra_data = read_from_s3(s3_bucket_extra_data, file_name, s3_client)\n",
    "\n",
    "        # Asegurarse de que extra_data sea un DataFrame o establecerlo como DataFrame vacío si no es así\n",
    "        if not isinstance(extra_data, pd.DataFrame):\n",
    "            logging.error(f'Proyecto Meta {project_id}: {extra_data.get(\"error\", \"Error desconocido al leer datos extra.\")}')\n",
    "            extra_data = pd.DataFrame()  # Crea un DataFrame vacío para permitir la creación de full_data\n",
    "\n",
    "        full_data = merge_data_if_needed(project_data, extra_data)\n",
    "        meta_parquet = df_to_parquet(full_data)\n",
    "        upload_project_parquet_to_s3(project_id, meta_parquet, s3_client, bucket_name, 'meta')\n",
    "        logging.info(f'Proyecto Meta {project_id}: Datos subidos correctamente a S3.')\n",
    "    else:\n",
    "        logging.warning(f'Proyecto Meta {project_id}: No se encontraron datos.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_parquets_meta_for_projects(project_list, s3_client):\n",
    "    s3_bucket_extra_data = 'siwaexperiments'\n",
    "    projects_parquet_key = 'projects/projects_record/meta.parquet'\n",
    "    bucket_name = \"siwaparquets\"\n",
    "\n",
    "    updated_projects_df = read_updated_projects(s3_client, bucket_name, projects_parquet_key)\n",
    "\n",
    "    for project_id in project_list:\n",
    "        if project_id in updated_projects_df:\n",
    "            logging.info(f'Proyecto meta {project_id}: El archivo ya está en S3 y no se subirá nuevamente.')\n",
    "            continue\n",
    "        process_project(project_id, s3_client, s3_bucket_extra_data, bucket_name)\n",
    "    return \"Proceso completado con éxito.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects = get_projectid_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Proceso completado con éxito.'"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upload_parquets_otus_for_projects(projects, s3_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Proceso completado con éxito.'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upload_parquets_meta_for_projects(projects, s3_client)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "athena-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
