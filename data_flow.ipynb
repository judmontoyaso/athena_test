{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import sqlalchemy as sa \n",
    "from sqlalchemy import  MetaData\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.engine.url import URL\n",
    "from sqlalchemy.schema import MetaData\n",
    "from sqlalchemy.engine.url import URL\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from dotenv import load_dotenv\n",
    "from base import OtuCount, FeatureCountExtendedView\n",
    "import logging\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar el logging\n",
    "logging.basicConfig(filename='otus_projects_upload.log', level=logging.INFO, \n",
    "                    format='%(asctime)s:%(levelname)s:%(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargar variables de entorno\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de la conexión a la base de datos\n",
    "def get_engine():\n",
    "    url = URL.create(\n",
    "        drivername=os.getenv('DB_DRIVER'),\n",
    "        host=os.getenv('DB_HOST'),\n",
    "        port=os.getenv('DB_PORT'),\n",
    "        database=os.getenv('DB_NAME'),\n",
    "        username=os.getenv('DB_USER'),\n",
    "        password=os.getenv('DB_PASSWORD')\n",
    "    )\n",
    "    engine =sa.create_engine(url)\n",
    "    return engine\n",
    "\n",
    "engine = get_engine()\n",
    "# Creación de MetaData y Declarative Base\n",
    "metadata = MetaData()\n",
    "Base = declarative_base(metadata=metadata)\n",
    "# Creación de la sesión\n",
    "Session = sessionmaker(bind=get_engine())\n",
    "session = Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def client(client):\n",
    "    return boto3.client(\n",
    "    client,\n",
    "    aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),\n",
    "    aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY'),\n",
    "    region_name=os.getenv('S3_REGION')\n",
    "    )\n",
    "# Configurar el cliente de s3\n",
    "s3_client = client('s3')\n",
    "# Configurar el cliente de Athena\n",
    "athena_client = client('athena')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pivotear_datos(df):\n",
    "    \"\"\"\n",
    "    Pivotea los datos de un DataFrame.\n",
    "\n",
    "    Parámetros:\n",
    "    df (DataFrame): DataFrame que contiene las columnas 'value', 'otu' y 'sampleId'.\n",
    "\n",
    "    Devuelve:\n",
    "    DataFrame: DataFrame pivoteado con 'otu' como índice y 'sampleId' como columnas.\n",
    "    \"\"\"\n",
    "\n",
    "    # Crear una tabla pivote\n",
    "    datos_pivoteados = (\n",
    "        df.pivot_table(values='value', index='otu', columns='sampleId')\n",
    "          .fillna(0)\n",
    "          .rename_axis(index=None, columns=None)\n",
    "    )\n",
    "\n",
    "    # Restablecer el índice\n",
    "    datos_pivoteados = datos_pivoteados.reset_index()\n",
    "\n",
    "    return datos_pivoteados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_project_parquet_to_s3(projectid, otu_parquet, s3_client, bucket_name):\n",
    "    \"\"\"\n",
    "    Sube un archivo Parquet a un bucket de S3 específico basado en el ID del proyecto.\n",
    "\n",
    "    :param projectid: El ID del proyecto para el cual se subirá el archivo.\n",
    "    :param otu_parquet: El contenido del archivo Parquet o la ruta al archivo a subir.\n",
    "    :param s3_client: La instancia del cliente S3 para realizar la subida.\n",
    "    \"\"\"\n",
    "    file_name = f\"otu/{projectid}/{projectid}.parquet\"\n",
    "    try:\n",
    "        s3_client.put_object(Bucket=bucket_name, Key=file_name, Body=otu_parquet)\n",
    "        return {\"message\": \"Archivo subido exitosamente\"}\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_parquet(df):\n",
    " # df_otu_con_project es tu DataFrame\n",
    "    parquet_buffer = df.to_parquet( engine='pyarrow', compression='snappy')\n",
    "    return parquet_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_projectid_all():\n",
    "    \"\"\"Consulta a la tabla FeatureCountExtendedView para obtener una lista de IDs de proyectos existentes.\"\"\"\n",
    "    # Ejecutar la consulta directamente y obtener los projectId como un dataframe\n",
    "    df = pd.read_sql(session.query(FeatureCountExtendedView.projectId).statement, session.bind)\n",
    "    \n",
    "    # Filtrar los projectId nulos y obtener una lista única de projectId\n",
    "    project_ids = df['projectId'].dropna().unique().tolist()\n",
    "    \n",
    "    return project_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects = get_projectid_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def get_feature_otu(projectId):\n",
    "    \"\"\"Consulta a la tabla otu para obtener los datos de un project especifico\"\"\"\n",
    "    query = session.query(\n",
    "        FeatureCountExtendedView.value, \n",
    "        FeatureCountExtendedView.otu, \n",
    "        FeatureCountExtendedView.sampleId\n",
    "    ).filter(FeatureCountExtendedView.projectId == projectId)\n",
    "    df = pd.read_sql(query.statement, session.bind)\n",
    "    \n",
    "    return df.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_parquets_otus_for_projects(project_list, s3_client):\n",
    "    \"\"\"\n",
    "    Procesa los datos de OTUs para cada proyecto en la lista que no se han actualizado previamente,\n",
    "    convierte los datos a formato Parquet, y los sube a un almacenamiento S3. Actualiza un archivo Parquet\n",
    "    en S3 que mantiene un registro de los proyectos procesados.\n",
    "\n",
    "    Args:\n",
    "        project_list (list): Lista de IDs de proyectos a procesar.\n",
    "        s3_client: Cliente de S3 para realizar operaciones de S3.\n",
    "        s3_bucket (str): Nombre del bucket de S3 donde se almacenan los archivos.\n",
    "\n",
    "    Returns:\n",
    "        str: Mensaje indicando que la operación fue exitosa.\n",
    "    \"\"\"\n",
    "    # Define la clave del archivo Parquet en S3 que mantiene el registro de proyectos subidos\n",
    "    s3_bucket = 'siwaparquets'\n",
    "    projects_parquet_key = 'projects/projects_record.parquet'\n",
    "\n",
    "    try:\n",
    "        # Intentar leer el archivo Parquet desde S3 que contiene los proyectos ya procesados\n",
    "        response = s3_client.get_object(Bucket=s3_bucket, Key=projects_parquet_key)\n",
    "\n",
    "        # Read the content of the response into a BytesIO buffer\n",
    "        buffer = BytesIO(response['Body'].read())\n",
    "\n",
    "        # Use the buffer to read the Parquet file\n",
    "        updated_projects_df = pd.read_parquet(buffer)\n",
    "        updated_projects_set = set(updated_projects_df['project_id'])\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"No se pudo leer el archivo de proyectos actualizados desde S3: {e}\")\n",
    "        updated_projects_set = set()\n",
    "\n",
    "    for project_id in project_list:\n",
    "        if project_id not in updated_projects_set:\n",
    "            try:\n",
    "                project_data = pd.DataFrame(get_feature_otu(project_id))\n",
    "                if not project_data.empty:\n",
    "                    otu_pivot = pivotear_datos(project_data)\n",
    "                    otu_parquet = df_to_parquet(otu_pivot)\n",
    "                    upload_project_parquet_to_s3(project_id, otu_parquet, s3_client, s3_bucket)\n",
    "                    logging.info(f'Proyecto {project_id}: Datos subidos correctamente a S3.')\n",
    "\n",
    "                    # Agregar el proyecto al conjunto de proyectos actualizados\n",
    "                    updated_projects_set.add(project_id)\n",
    "                else:\n",
    "                    logging.warning(f'Proyecto {project_id}: No se encontraron datos.')\n",
    "            except Exception as e:\n",
    "                logging.error(f'Proyecto {project_id}: Error al procesar - {e}')\n",
    "                continue\n",
    "        else: \n",
    "            logging.info(f'Proyecto {project_id}: El archivo no se subio por que ya esta en s3.')\n",
    "\n",
    "    # Actualizar el archivo Parquet con la lista de proyectos subidos\n",
    "    updated_projects_df = pd.DataFrame(list(updated_projects_set), columns=['project_id'])\n",
    "    updated_projects_parquet = df_to_parquet(updated_projects_df)  # Asegúrate de que esta función devuelva los datos en formato adecuado para `put_object`\n",
    "    s3_client.put_object(Bucket=s3_bucket, Key=projects_parquet_key, Body=updated_projects_parquet)\n",
    "\n",
    "    return \"Proceso completado con éxito.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Proceso completado con éxito.'"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upload_parquets_otus_for_projects(projects, s3_client)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "athena-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
